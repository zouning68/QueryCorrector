import re, logging, traceback, copy, json, codecs
from collections import Counter
from config import config
from Pinyin2Hanzi import DefaultDagParams, dag

PUNCTUATION_LIST = ".ã€‚,ï¼Œ,ã€?ï¼Ÿ:ï¼š;ï¼›{}[]ã€ã€‘â€œâ€˜â€™â€ã€Šã€‹/!ï¼%â€¦â€¦ï¼ˆï¼‰<>@#$~^ï¿¥%&*\"\'=+-_â€”â€”ã€Œã€"
SPECIAL_WORDS = ['c++','cocos2d-x','.net','--','node.js','c/s','c#','unity3d','2d','3d','cocos2d']
BLACK_WORDS = ['andrid','exel','jav','andriod','andrioid','andrio','andriord','androd','bap','å¼€æ³•','servier', \
               'taradata','teredata','bootstap','wmware','andiord','andrior','adriod','androi','linex','javaa','javacript', \
               'myibatis','jquey','javal','tomat','pafa','oracl','websersive','javs','javascrip','vware','websevice','javascrit', \
               'format']

#**********************************************************************************************************************#
class ErrorType(object):
    confusion, word, term, english = 'confusion', 'word', 'term', 'english'

dagParams = DefaultDagParams()
def pinyin2hanzi(pinyinList, num=3):
    result = dag(dagParams, pinyinList, path_num=num)
    return result
a = pinyin2hanzi(["meitu"])

names = [e.strip() for e in codecs.open(config.baijiaxing, encoding='utf8').readlines() if e.strip() != '']
def is_name(text):
    if not text or text.strip() in PUNCTUATION_LIST: return False
    text = str(text)    #; aa=text[0]
    if len(text) > 2 and text[:2] in names: return True
    if len(text) in [1, 2, 3] and text[0] in names: return True
    else: return False
a=is_name("é”œæ™“æ•")

re_ch = re.compile(u"([\u4e00-\u9fa5])",re.S)
re_en = re.compile(u"([a-zA-Z]+|[0-9]+k[\+]*)",re.S)
re_salary = re.compile(u"([0-9]+k[\+]*)",re.S)

def en_split(text):
    text = text.lower()
    res = []
    for w in SPECIAL_WORDS:
        if text.find(w) < 0: continue
        text = text.replace(w, ' '+w+' ')
    seg_text = text.strip().split()
    for w in seg_text:
        if w in SPECIAL_WORDS:
            res.append(w)
        else:
            for e in re_en.split(w):
                if e in ['', ' ']: continue
                res.append(e)
    return res
que = "advc#montage+æ·±åœ³c++c/s5k"
a = re_en.split(que)
#aa = en_split(que)

def is_alphabet_string(string):     # åˆ¤æ–­æ˜¯å¦å…¨éƒ¨ä¸ºè‹±æ–‡å­—æ¯
    string = string.lower()
    for c in string:
        if c < 'a' or c > 'z':
            return False
    return True

def need_correct_pinying(string):     # åˆ¤æ–­æ˜¯å¦å…¨éƒ¨ä¸ºè‹±æ–‡å­—æ¯
    string = string.lower(); #a = en_split(string)
    for c in en_split(string):
        if c in PUNCTUATION_LIST: continue
        #aa=pinyin2hanzi([c])
        if pinyin2hanzi([c]):
            return True
    return False
#a=need_correct_pinying("dongå¼€å‘")

def Q2B(uchar):     # å…¨è§’è½¬åŠè§’
    inside_code = ord(uchar)
    if inside_code == 0x3000:
        inside_code = 0x0020
    else:
        inside_code -= 0xfee0
    if inside_code < 0x0020 or inside_code > 0x7e:  # è½¬å®Œä¹‹åä¸æ˜¯åŠè§’å­—ç¬¦è¿”å›åŸæ¥çš„å­—ç¬¦
        return uchar
    return chr(inside_code)

def stringQ2B(ustring):     # æŠŠå­—ç¬¦ä¸²å…¨è§’è½¬åŠè§’
    return "".join([Q2B(uchar) for uchar in ustring])

def uniform(ustring):       # æ ¼å¼åŒ–å­—ç¬¦ä¸²ï¼Œå®Œæˆå…¨è§’è½¬åŠè§’ï¼Œå¤§å†™è½¬å°å†™çš„å·¥ä½œ
    return stringQ2B(ustring).lower()

def is_chinese(uchar):      # åˆ¤æ–­ä¸€ä¸ªunicodeæ˜¯å¦æ˜¯æ±‰å­—
    if u'\u4e00' <= uchar <= u'\u9fa5':
        return True
    else:
        return False

def is_chinese_string(string):      # åˆ¤æ–­æ˜¯å¦å…¨ä¸ºæ±‰å­—
    for c in string:
        if not is_chinese(c):
            return False
    return True

def filter_entity(curr_word, before_string, after_string):
    for e in ["å…¬å¸", "university", "å¤§å­¦", "é›†å›¢", "æœºæ„" ,"ç§‘æŠ€"]:
        if e in after_string.lower() or e in before_string.lower(): return True
    return False

def edit_distance_word(word, char_set):     # all edits that are one edit away from 'word'
    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]
    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]
    replaces = [L + c + R[1:] for L, R in splits if R for c in char_set]
    return set(transposes + replaces)
#**********************************************************************************************************************#

def n_gram_words(text, n_gram=4, return_list=False):
    # n_gram å¥å­çš„è¯é¢‘å­—å…¸
    words, words_freq = [], dict()
    try:
        # 1 - n gram
        for i in range(1, n_gram + 1):
            words += [text[j: j + i] for j in range(len(text) - i + 1)]
        words_freq = dict(Counter(words))
    except Exception as e:
        logging.warning('n_gram_words_err=%s' % repr(e)); print(traceback.format_exc())
    if return_list: return words
    else: return words_freq

def rmPunct(line):
    line = re.sub(r"[â˜…\n-â€¢ï¼ï¼»â€¦\tã€ï¼‹ï‚Ÿï¼†ã€€â•ï¼Š]+", "", line)
    line = re.sub(r"[,\./;'\[\]`!@#$%\^&\*\(\)=\+<> \?:\"\{\}-]+", "", line)
    line = re.sub(r"[ã€\|ï¼Œã€‚ã€Šã€‹ï¼›â€œâ€â€˜â€™ï¼›ã€ã€‘ï¿¥ï¼ï¼Ÿï¼ˆï¼‰ï¼š ï½]+", "", line)
    line = re.sub(r"[~/'\"\(\)\^\.\*\[\]\?\\]+", "", line)
    return line
a=rmPunct("æ¶ˆè´¹è€…/é¡¾å®¢wordã€excelã€pptã€visioã€xmind")

def clean_query(query):
    query = re.sub(u"[&$ï¿¥ï½ï¿½|ï¼ ï¼Ÿï¼ï¼ï¼œï¼›!ï½œï½›ï¼¼ï¼½ï¼»ï¼ï¼ï¼‹ï¼Š*ï¼†ï¼…ï¼ƒï¼‚ï¼ï¬ğŸŒï¼ï¹’ï©…ï¤Šïƒ˜ï·ï®ïµ]{1,}|[.#-]{2,}|[+]{3,}|[0-9]*%", "", query)
    query = re.sub(u"[\\/ã€ï¼Œ]+", ",", query)
    query = re.sub(u"[ï¼ˆ]+", "(", query)
    query = re.sub(u"[ï¼‰]+", ")", query)
    query = re.sub(u"[ã€ã€‘â—|â€œâ€^H*]+", " ", query)
    query = re.sub(u"[ï¼š]+", ":", query)
    query = re.sub(u"[ ~]+", " ", query)
    query = query.lstrip(",")
    query = query.rstrip(",")
    query = query.strip().lower()
    return query
aa=clean_query("####æ–‡æ—…åœ°.......äº§é›†ï½œå›¢.ï¼¼.....")

def normal_qeury(text):
    re_ch = re.compile("([\u4e00-\u9fa5])", re.S)
    re_digital = re.compile("[0-9]{2,}", re.S)
    re_longdigital = re.compile("[0-9]{5,}", re.S)
    re_valid = re.compile("[ç®€å†]", re.S)
    digital = re_digital.findall(text)
    chinese = re_ch.findall(text)
    valid = re_valid.findall(text)
    long_digital = re_longdigital.findall(text)
    if long_digital: return False
    elif digital and not chinese: return False
    elif len(text) > 20 or len(text) < 2 or valid: return False
    else: return True
a=normal_qeury(clean_query("æœç‹ç•…æ¸¸17173"))

if __name__ == '__main__':
    a = normal_qeury("k12d2d2")
    A = clean_query("å¸‚åœºé”€å”®^H*..")
    a=is_chinese_string("å¼€æ³•")
    pass
